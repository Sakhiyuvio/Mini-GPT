# ğŸª Mini-GPT: A Tiny Language Model Trained on Sci-Fi Classics

This project is a small-scale GPT-style language model trained on a corpus of classic science fiction literature from the public domain. The goal is to explore how a transformer-based model can learn the patterns, style, and imagination found in early sci-fi texts.

## ğŸš€ Project Overview

- **Model Type**: Miniature GPT (similar to GPT-2 transformer architecture, but much smaller)
- **Training Data**: Selected public domain sci-fi books from Project Gutenberg, including:
  - *The War of the Worlds* by H.G. Wells
  - *A Princess of Mars* by Edgar Rice Burroughs
  - *Frankenstein* by Mary Shelley
  - *Flatland* by Edwin A. Abbott
  - *The Time Machine* by H.G. Wells
- **Training Objective**: Next-token prediction (language modeling)
- **Framework**: PyTorch

## ğŸ“ License

All training data is sourced from public domain texts (via [Project Gutenberg](https://www.gutenberg.org/)).